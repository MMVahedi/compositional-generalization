https://aclanthology.org/2024.acl-long.455.pdf#:~:text=Empirically%2C%20we%20show%20SQ%02Transformer%20achieves,patterns%2C%20alto%02gether%20leading%20to%20improved

[Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference](https://proceedings.neurips.cc/paper_files/paper/2024/file/df22a19686a558e74f038e6277a51f68-Paper-Conference.pdf)

[Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks](https://arxiv.org/pdf/1711.00350)

[Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation](https://aclanthology.org/2024.acl-long.351.pdf)

[The Devil is in the Details: Simple Tricks Improve Systematic Generalization of Transformers](https://aclanthology.org/2021.emnlp-main.49.pdf)

[Compositional Generalization for Neural Semantic Parsing via Span-level Supervision](https://aclanthology.org/2021.naacl-main.225.pdf)

[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](https://arxiv.org/pdf/2407.04543)

[LEARNING SYNTAX WITHOUT PLANTING TREES: UNDERSTANDING HIERARCHICAL GENERALIZATION IN TRANSFORMERS](https://arxiv.org/pdf/2404.16367v3)

[Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale](https://aclanthology.org/2022.tacl-1.81.pdf)

[Differentiable Tree Operations Promote Compositional Generalization](https://proceedings.mlr.press/v202/soulos23a/soulos23a.pdf#:~:text=Tensor%20Product%20Representation%20,also%20known%20as)