در این جلسه مقاله زیر ارائه شد که درباره OOD Detection & Model Evaluation بود. حرف خلاصه مقاله این بود که در زمان آموزش، هر چقدر یک نورون دفعات بیشتری و برای نمونه‌های بیشتری فعال شود انتظار می‌رود مه هنگام تست و داده OOD عملکرد عجیبی از خودش بروز ندهد و باعث شود که لیبل برای داده تست اشتباه پیش‌بینی شود. دد واقع هر نورون که فضای coverage برای خودش دارد که برای داده‌های داخل اون عملکرد درستی دارد و برای خارج آن ممکن است باعث اشتباه شود و هرجقدر تعداد داده‌هایی که برای آنها در اموزش فعال شده‌است بیشتر باشد، این ناحیه coverage گسترده‌تر می‌شود.
یک نکته‌ای که من (واحدی) گفتم این بود که خب این با تعریف modular ای که تا الان برای تسک‌های compositional می‌گفتیم و disentanglement در representation learning در تناقض هست. چون در اون موارد هر بخش representation و یا network یکی از وظایف یا ویژگی ها را مدل می‌کند تا خاصیت تعمیم‌پذیری بروز کند. چون این شکلی هر بخش فقط برای بخشی از ورودی فعال می‌شود. نظر خانم دکتر این بود که بحث من جدا از این صحبت هست و این فرض صرفا برای جلوگیری مدل از memorization هست و انگار داره اون مدل‌هایی که این کار رو میکنند و در نتیجه روی OOD ضعیف هستند را حذف می‌کند..

### [Neuron Activation Coverage: Rethinking Out-of-Distribution Detection and Generalization](https://arxiv.org/pdf/2306.02879)

این مقاله ممکنه در آینده به چه دردی بخوره؟ این روش بررسی تاثیر‌گذاری neuron ممکنه توی تفسیر کردن مدل‌ها بدرد بخوره. ار طرفی تعریف توزیع برای فعال شده neuron ها در محدوده برد تابع activation میتونه ایده‌ی قابل apply کردن به مسئله خودمون بشه.

بحث دیگه‌ای که شد این بود که به نظر میاد tokenization میتونه روی قابلیت ‌compositionality مدل تائثیرگذار باشه. ما اکثرا توی nlp از روی دفعات وقوع ترکیب داده‌ها token های جدید و بزرگتر که از ترکیب اجزای کوچیکتر تشکیل می‌شوند تولید میکنیم تا مجموعه توکن‌ها ساخته شود اینکار می‌تواند باعث شود قدرت تعمیم‌پذیری مدل کاهش یابد چون ما مثلا ab ac ad رو دیدیم توی داده‌های ولی ae رو ندیدیم در حالی که ذاتا میتونن ترکیب بشوند ولی چون پر تکرار نیست مدل ممکنه درست ترکیب رو یاد نگیره و بره سراغ حذف خود ab ac ad. در مقایسه با یک مسئله symbolic که symbol ها به تمام اشکال می‌توانند با هم ترکیب شوند بهتر معنی مید‌هد.
دقت کنید که ما با تغییر tokenizer فضای ورودی و خروجی را تغییر می‌دهیم و شاید بتوان با اینکار به فضایی رفت که composition در آن معنا درست پیدا می‌کند و مدل راحت‌تر و با داده کمتر می‌تواند قاعده‌ها را یاد بکیرد.